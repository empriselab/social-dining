{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca329be-87c8-4b18-9f6e-1beb8ebc06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ab566-87cb-4a33-a34c-73f95ec8e739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40305feb-75e9-473c-88d8-22565b06a209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████████████▊                                                                                                                                                                                                                                                                                                                                    | 379/9316 [00:38<12:01, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████████████████████▏                                                                                                                                                                                                                                                                                                                | 916/9316 [01:31<11:15, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                      | 1176/9316 [01:58<17:23,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                                   | 2153/9316 [03:58<09:44, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                | 2225/9316 [04:03<09:47, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                             | 2323/9316 [04:12<10:51, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                      | 2500/9316 [04:30<14:44,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                              | 3602/9316 [06:43<07:39, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                         | 3762/9316 [06:56<07:15, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 4874/9316 [08:41<08:34,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                              | 4941/9316 [08:49<11:18,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                              | 5370/9316 [09:28<05:25, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                          | 5500/9316 [09:39<05:08, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5608/9316 [09:48<05:31, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                        | 5996/9316 [10:24<05:04, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                             | 6744/9316 [11:46<03:43, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9316/9316 [16:04<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168.3081545829773\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 310>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m total \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(total)\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28mprint\u001b[39m((total\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mmain\u001b[49m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m9316\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate_op\u001b[39m():\n\u001b[1;32m    313\u001b[0m     p_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/positive_op/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# Openpose features\n",
    "def get_openpose_from_json(video_id, start_frame, window_size):\n",
    "    \"\"\"\n",
    "    :param video_id: the id of video you want to get openpose\n",
    "    :param start_frame: the frame when the bite action starts\n",
    "    :param window_size: the window size of each sample in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(f\"/home/aa2375/social-dining/data/openpose/processed/{video_id}_{start_frame}.npy\"):\n",
    "        d = np.load(f\"/home/aa2375/social-dining/data/openpose/processed/{video_id}_{start_frame}.npy\", allow_pickle=True).item()\n",
    "        body_sample = d['body']\n",
    "        face_sample = d['face']\n",
    "        \n",
    "    else:\n",
    "        path = '/home/aa2375/social-dining/data/openpose/' + str(video_id)\n",
    "        # frame_list = os.listdir(path)\n",
    "        frame_list = glob.glob(path + '/*')\n",
    "        frame_list.sort()\n",
    "        # print(\"len:\", len(frame_list))\n",
    "\n",
    "        b_x, b_y, b_c = [], [], []\n",
    "        lh_x, lh_y, lh_c = [], [], []\n",
    "        rh_x, rh_y, rh_c = [], [], []\n",
    "        f_x, f_y, f_c = [], [], []\n",
    "\n",
    "        start_id = start_frame\n",
    "        end_id = start_frame + 30 * window_size\n",
    "\n",
    "        for i in range(start_frame, end_id):\n",
    "            with open(frame_list[i]) as f:\n",
    "                pose_frame = json.load(f)\n",
    "            if not pose_frame['people']:  # if no body detected:\n",
    "                body_joints = np.empty(25 * 3)\n",
    "                body_joints[:] = np.nan\n",
    "                face_joints = np.empty(70 * 3)\n",
    "                face_joints[:] = np.nan\n",
    "            else:\n",
    "                body_joints = pose_frame['people'][0]['pose_keypoints_2d']\n",
    "                face_joints = pose_frame['people'][0]['face_keypoints_2d']\n",
    "\n",
    "            a = 0\n",
    "            while a < (len(body_joints)):\n",
    "                b_x.append(body_joints[a])\n",
    "                b_y.append(body_joints[a + 1])\n",
    "                b_c.append(body_joints[a + 1])\n",
    "                a += 3\n",
    "            a = 0\n",
    "            while a < (len(face_joints)):\n",
    "                f_x.append(face_joints[a])\n",
    "                f_y.append(face_joints[a + 1])\n",
    "                f_c.append(face_joints[a + 1])\n",
    "                a += 3\n",
    "\n",
    "        body_op_x = np.array(b_x).reshape(window_size * 30, -1)\n",
    "        body_op_y = np.array(b_y).reshape(window_size * 30, -1)\n",
    "        body_op_c = np.array(b_c).reshape(window_size * 30, -1)\n",
    "\n",
    "        # remove keypoints on legs and feet\n",
    "        body_op_x = np.delete(body_op_x, [10, 11, 13, 14, 18, 19, 20, 21, 22, 23, 24], 1)\n",
    "        body_op_y = np.delete(body_op_y, [10, 11, 13, 14, 18, 19, 20, 21, 22, 23, 24], 1)\n",
    "        body_op_c = np.delete(body_op_c, [10, 11, 13, 14, 18, 19, 20, 21, 22, 23, 24], 1)\n",
    "\n",
    "\n",
    "        face_op_x = np.array(f_x).reshape(window_size * 30, -1)\n",
    "        face_op_y = np.array(f_y).reshape(window_size * 30, -1)\n",
    "        face_op_c = np.array(f_c).reshape(window_size * 30, -1)\n",
    "\n",
    "        body_sample = np.concatenate([body_op_x, body_op_y], axis=1)\n",
    "        face_sample = np.concatenate([face_op_x, face_op_y], axis=1)\n",
    "\n",
    "        np.save(f'/home/aa2375/social-dining/data/openpose/processed/{video_id}_{start_frame}.npy', {'body':body_sample, 'face':face_sample})\n",
    "    \n",
    "    \n",
    "    return np.array(body_sample).astype(float), np.array(face_sample).astype(float)\n",
    "\n",
    "\n",
    "def map_samples_to_labels(elan_label_path):\n",
    "    elan_label = pd.read_csv(elan_label_path, index_col=0)\n",
    "\n",
    "    vid_left_list = [] # left\n",
    "    vid_right_list = [] # right\n",
    "    \n",
    "    all_samples = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(elan_label))):\n",
    "        vid_main = elan_label['Name'].iloc[i]\n",
    "        start_frame = elan_label['Start Frame'].iloc[i]\n",
    "\n",
    "        # this determines the left and right people!\n",
    "        if vid_main[-1] == '1':\n",
    "            vid_left_list.append(vid_main[0:-1]+str(3))\n",
    "            vid_right_list.append(vid_main[0:-1] + str(2))\n",
    "        elif vid_main[-1] == '2':\n",
    "            vid_left_list.append(vid_main[0:-1] + str(1))\n",
    "            vid_right_list.append(vid_main[0:-1] + str(3))\n",
    "        elif vid_main[-1] == '3':\n",
    "            vid_left_list.append(vid_main[0:-1] + str(2))\n",
    "            vid_right_list.append(vid_main[0:-1] + str(1))\n",
    "        else: # if it's 0 (the scene camera)\n",
    "            continue\n",
    "            \n",
    "        # get openpose data (for all people)\n",
    "        # expensive so using futures\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=18) as executor:\n",
    "            futures = []\n",
    "                \n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_openpose_from_json, vid_main, start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_openpose_from_json, vid_left_list[-1], start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_openpose_from_json, vid_right_list[-1], start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_gaze_from_csv, vid_main, start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_gaze_from_csv, vid_left_list[-1], start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_gaze_from_csv, vid_right_list[-1], start_frame, 6\n",
    "                )\n",
    "            )   \n",
    "            \n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_time_from_csv, vid_main, start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_time_from_csv, vid_left_list[-1], start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_time_from_csv, vid_right_list[-1], start_frame, 6\n",
    "                )\n",
    "            )   \n",
    "            \n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_count_from_csv, vid_main, start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_count_from_csv, vid_left_list[-1], start_frame, 6\n",
    "                )\n",
    "            )\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    get_count_from_csv, vid_right_list[-1], start_frame, 6\n",
    "                )\n",
    "            )  \n",
    "            \n",
    "            \n",
    "                \n",
    "            concurrent.futures.wait(futures)\n",
    "                \n",
    "            body_main, face_main = futures[0].result()\n",
    "            body_left, face_left = futures[1].result()\n",
    "            body_right, face_right = futures[2].result()\n",
    "\n",
    "            gaze_main, headpose_main = futures[3].result()\n",
    "            gaze_left, headpose_left = futures[4].result()\n",
    "            gaze_right, headpose_right = futures[5].result()\n",
    "            \n",
    "            time_since_last_lifted, time_since_last_bite, time_since_last_bite_zeroed = futures[6].result()\n",
    "            time_since_last_lifted_left, time_since_last_bite_left, time_since_last_bite_zeroed_left = futures[7].result()\n",
    "            time_since_last_lifted_right, time_since_last_bite_right, time_since_last_bite_zeroed_right = futures[8].result()\n",
    "            \n",
    "            num_lifted, num_bites = futures[9].result()\n",
    "            num_lifted_left, num_bites_left = futures[10].result()\n",
    "            num_lifted_right, num_bites_right = futures[11].result()\n",
    "                \n",
    "            # body_main, face_main = get_openpose_from_json(vid_main, start_frame, 6)\n",
    "            # body_left, face_left = get_openpose_from_json(vid_left_list[-1], start_frame, 6)\n",
    "            # body_right, face_right = get_openpose_from_json(vid_right_list[-1], start_frame, 6)\n",
    "        # body_main = face_main = body_left = face_left = body_right = face_right = np.random.random((180, 28))\n",
    "        \n",
    "        # get speaking data (for all people)\n",
    "        one, two, three = get_speaking_from_csv(vid_main[:2], start_frame, 6)\n",
    "        if vid_main[-1] == '1':\n",
    "            main_speaking = one\n",
    "            left_speaking = three\n",
    "            right_speaking = two\n",
    "        elif vid_main[-1] == '2':\n",
    "            main_speaking = two\n",
    "            left_speaking = one\n",
    "            right_speaking = three\n",
    "        elif vid_main[-1] == '3':\n",
    "            main_speaking = three\n",
    "            left_speaking = two\n",
    "            right_speaking = one\n",
    "        \n",
    "        # get gaze/headpose data (for all people)\n",
    "        # gaze_main, headpose_main = get_gaze_from_csv(vid_main, start_frame, 6)\n",
    "        # gaze_left, headpose_left = get_gaze_from_csv(vid_left_list[-1], start_frame, 6)\n",
    "        # gaze_right, headpose_right = get_gaze_from_csv(vid_right_list[-1], start_frame, 6)\n",
    "\n",
    "        # get time feats (only for the main person). we'll save all sides though\n",
    "        # time_since_last_lifted, time_since_last_bite, time_since_last_bite_zeroed = get_time_from_csv(vid_main, start_frame, 6)\n",
    "        # time_since_last_lifted_left, time_since_last_bite_left, time_since_last_bite_zeroed_left = get_time_from_csv(vid_left_list[-1], start_frame, 6)\n",
    "        # time_since_last_lifted_right, time_since_last_bite_right, time_since_last_bite_zeroed_right = get_time_from_csv(vid_right_list[-1], start_frame, 6)\n",
    "\n",
    "        # get count feats (only for the main person). we'll save all of them though\n",
    "        # num_lifted, num_bites = get_count_from_csv(vid_main, start_frame, 6)\n",
    "        # num_lifted_left, num_bites_left = get_count_from_csv(vid_left_list[-1], start_frame, 6)\n",
    "        # num_lifted_right, num_bites_right = get_count_from_csv(vid_right_list[-1], start_frame, 6)\n",
    "        \n",
    "        person_main = {'body':body_main, 'face':face_main, 'speaking':main_speaking, 'gaze':gaze_main, 'headpose':headpose_main, \n",
    "                           'time_since_last_lifted':time_since_last_lifted, 'time_since_last_bite':time_since_last_bite, 'time_since_last_bite_zeroed':time_since_last_bite_zeroed, \n",
    "                           'num_lifted':num_lifted, 'num_bites':num_bites}\n",
    "        \n",
    "        person_left = {'body':body_left, 'face':face_left, 'speaking':left_speaking, 'gaze':gaze_left, 'headpose':headpose_left, \n",
    "                           'time_since_last_lifted':time_since_last_lifted_left, 'time_since_last_bite':time_since_last_bite_left, 'time_since_last_bite_zeroed':time_since_last_bite_zeroed_left, \n",
    "                           'num_lifted':num_lifted_left, 'num_bites':num_bites_left}\n",
    "        person_right = {'body':body_right, 'face':face_right, 'speaking':right_speaking, 'gaze':gaze_right, 'headpose':headpose_right, \n",
    "                           'time_since_last_lifted':time_since_last_lifted_right, 'time_since_last_bite':time_since_last_bite_right, 'time_since_last_bite_zeroed':time_since_last_bite_zeroed_right, \n",
    "                           'num_lifted':num_lifted_right, 'num_bites':num_bites_right}\n",
    "        \n",
    "        sample = {'main':person_main, 'left':person_left, 'right':person_right, 'label':elan_label['label'].iloc[i], 'main_id':vid_main, 'start_frame':start_frame}\n",
    "        \n",
    "        all_samples.append(sample)\n",
    "\n",
    "    np.save('all_samples.npy', all_samples)\n",
    "    \n",
    "def organize_data(person_features, global_features):\n",
    "    samples = np.load('all_samples.npy', allow_pickle=True)\n",
    "    person_main = []\n",
    "    person_left = []\n",
    "    person_right = []\n",
    "    global_feats = []\n",
    "    issue_count = 0\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(samples))):\n",
    "        \n",
    "        if samples[i]['start_frame'] < 0: # this should happen 16 times\n",
    "            issue_count += 1\n",
    "            continue\n",
    "     \n",
    "        if ('speaking' in person_features) and ('09' in samples[i]['main_id']):\n",
    "            continue\n",
    "        person_main.append([])\n",
    "        person_left.append([])\n",
    "        person_right.append([])\n",
    "        global_feats.append([])\n",
    "        labels.append(samples[i]['label'])\n",
    "\n",
    "        for feature in person_features:          \n",
    "            main_feature = samples[i]['main'][feature]\n",
    "            left_feature = samples[i]['left'][feature]\n",
    "            right_feature = samples[i]['right'][feature]\n",
    "            \n",
    "            if len(left_feature) != 180:\n",
    "                # this only happens once in 14_2, so we ensure the rest of the values are the same...\n",
    "                # just grow the size of it to 180\n",
    "                print(feature, samples[i]['main_id'], left_feature.shape)\n",
    "                diff = 180 - len(left_feature)\n",
    "                to_add = left_feature[-1].reshape(1, -1).repeat(diff, axis=0)\n",
    "                left_feature = np.concatenate([left_feature, to_add])\n",
    "            \n",
    "            person_main[-1].append(main_feature)\n",
    "            person_left[-1].append(left_feature)\n",
    "            person_right[-1].append(right_feature)\n",
    "\n",
    "        person_main[-1] = np.concatenate(person_main[-1], axis=1)\n",
    "        person_left[-1] = np.concatenate(person_left[-1], axis=1)\n",
    "        person_right[-1] = np.concatenate(person_right[-1], axis=1)\n",
    "\n",
    "                \n",
    "        for feature in global_features:\n",
    "            global_feats[-1].append(samples[i]['main'][feature])\n",
    "    print(issue_count)\n",
    "    return np.array(person_main), np.array(person_left), np.array(person_right), np.array(global_feats), np.array(labels)\n",
    "            \n",
    "import time\n",
    "start = time.time()\n",
    "# main, left, right, global_feats = organize_data(['body', 'face', 'gaze', 'headpose', 'speaking'], ['num_bites', 'time_since_last_bite'])\n",
    "# main, left, right, global_feats, labels = organize_data(['face', 'gaze', 'headpose', 'speaking'], ['num_bites', 'time_since_last_bite'])\n",
    "\n",
    "\n",
    "map_samples_to_labels('/home/aa2375/social-dining/src/preprocessing/labels.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "total = end - start\n",
    "print(total)\n",
    "print((total/len(main)) * 9316)\n",
    "\n",
    "def concatenate_op():\n",
    "    p_path = \"data/positive_op/\"\n",
    "    n_path = \"data/negative_op/\"\n",
    "    main_list = []\n",
    "    list_1 = []\n",
    "    list_2 = []\n",
    "    ops_main_positive = sorted(glob.glob(f'{p_path}/*_main.npy'))\n",
    "    ops_1_positive = sorted(glob.glob(f'{p_path}/*_1.npy'))\n",
    "    ops_2_positive = sorted(glob.glob(f'{p_path}/*_2.npy'))\n",
    "    ops_main_negative = sorted(glob.glob(f'{n_path}/*_main.npy'))\n",
    "    ops_1_negative = sorted(glob.glob(f'{n_path}/*_1.npy'))\n",
    "    ops_2_negative = sorted(glob.glob(f'{n_path}/*_2.npy'))\n",
    "\n",
    "    for i in range(len(ops_main_positive)):\n",
    "        main_list.append(np.load(ops_main_positive[i], allow_pickle=True))\n",
    "        list_1.append(np.load(ops_1_positive[i], allow_pickle=True))\n",
    "        list_2.append(np.load(ops_2_positive[i], allow_pickle=True))\n",
    "\n",
    "    for i in range(len(ops_main_negative)):\n",
    "        main_list.append(np.load(ops_main_negative[i], allow_pickle=True))\n",
    "        list_1.append(np.load(ops_1_negative[i], allow_pickle=True))\n",
    "        list_2.append(np.load(ops_2_negative[i], allow_pickle=True))\n",
    "\n",
    "    np.save(\"data/main_np.npy\", np.array(main_list))\n",
    "    np.save(\"data/dinner1_np.npy\", np.array(list_1))\n",
    "    np.save(\"data/dinner2_np.npy\", np.array(list_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103522bb-b6c4-43fd-a555-57189f6bfba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9316/9316 [00:00<00:00, 53892.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                 | 7718/27948 [00:06<00:15, 1277.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     48\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     49\u001b[0m                         get_openpose_from_json_pbar, job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m], job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m6\u001b[39m, pbar\n\u001b[1;32m     50\u001b[0m                     )\n\u001b[1;32m     51\u001b[0m                 )\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;66;03m# concurrent.futures.wait(futures) # I don't care about the output so we just end\u001b[39;00m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;66;03m# for future in concurrent.futures.as_completed(futures):\u001b[39;00m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;66;03m#     result = future.result()\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mprocess_all_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/aa2375/social-dining/src/preprocessing/labels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mprocess_all_op\u001b[0;34m(elan_label_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     38\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     39\u001b[0m         get_openpose_from_json_pbar, job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvid_main\u001b[39m\u001b[38;5;124m'\u001b[39m], job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m6\u001b[39m, pbar\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     43\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     44\u001b[0m         get_openpose_from_json_pbar, job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m], job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m6\u001b[39m, pbar\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     48\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     49\u001b[0m         get_openpose_from_json_pbar, job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m], job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m6\u001b[39m, pbar\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dining/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dining/lib/python3.8/concurrent/futures/thread.py:236\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 236\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dining/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/dining/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from threading import Lock\n",
    "\n",
    "pbar_mutex = Lock()\n",
    "\n",
    "def get_openpose_from_json_pbar(v, s, t, pbar):\n",
    "    get_openpose_from_json(v,s,t)\n",
    "    with pbar_mutex:\n",
    "        pbar.update(1)\n",
    "\n",
    "def process_all_op(elan_label_path):\n",
    "    elan_label = pd.read_csv(elan_label_path, index_col=0)\n",
    "\n",
    "    jobs = []\n",
    "    for i in tqdm.tqdm(range(len(elan_label))):\n",
    "        vid_main = elan_label['Name'].iloc[i]\n",
    "        start_frame = elan_label['Start Frame'].iloc[i]\n",
    "\n",
    "        # this determines the left and right people!\n",
    "        if vid_main[-1] == '1':\n",
    "            left = vid_main[0:-1]+str(3)\n",
    "            right = vid_main[0:-1] + str(2)\n",
    "        elif vid_main[-1] == '2':\n",
    "            left = vid_main[0:-1] + str(1)\n",
    "            right = vid_main[0:-1] + str(3)\n",
    "        elif vid_main[-1] == '3':\n",
    "            left = vid_main[0:-1] + str(2)\n",
    "            right = vid_main[0:-1] + str(1)\n",
    "        else: # if it's 0 (the scene camera)\n",
    "            continue\n",
    "            \n",
    "        jobs.append({'vid_main':vid_main, 'left':left, 'right':right, 'start_frame':start_frame})\n",
    "    print(\"Starting\")\n",
    "    with tqdm.tqdm(total=len(jobs)*3) as pbar:      \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            futures = []\n",
    "            for job in jobs:\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        get_openpose_from_json_pbar, job['vid_main'], job['start_frame'], 6, pbar\n",
    "                    )\n",
    "                )\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        get_openpose_from_json_pbar, job['left'], job['start_frame'], 6, pbar\n",
    "                    )\n",
    "                )\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        get_openpose_from_json_pbar, job['right'], job['start_frame'], 6, pbar\n",
    "                    )\n",
    "                )\n",
    "            # concurrent.futures.wait(futures) # I don't care about the output so we just end\n",
    "            # for future in concurrent.futures.as_completed(futures):\n",
    "            #     result = future.result()\n",
    "    \n",
    "process_all_op('/home/aa2375/social-dining/src/preprocessing/labels.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428d800-f52f-48ce-aca8-3be7caf97d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "main, left, right, global_feats, labels = organize_data(['face', 'gaze', 'headpose', 'speaking'], ['num_bites', 'time_since_last_bite'])\n",
    "left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d388b1-b4d2-445a-810b-b9a14c52957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2)\n",
      "(45, 2)\n"
     ]
    }
   ],
   "source": [
    "# gaze/headpose features\n",
    "\n",
    "def interpolate_gaze_headpose(df):\n",
    "    l = []\n",
    "    for i in range(1, len(df) - 1):\n",
    "        pre = df['name'].iloc[i-1]\n",
    "        cur = df['name'].iloc[i]\n",
    "        if int(pre) != int(cur) - 1:\n",
    "            diff = int(cur) - int(pre)\n",
    "            for j in range(diff):\n",
    "                should = int(pre) + j\n",
    "                l.append(int(should))\n",
    "        else:\n",
    "            l.append(df.iloc[i-1]['name'])\n",
    "    new_col = pd.DataFrame({\"new_name\": l})\n",
    "    df = df.set_index(\"name\")\n",
    "    new_col = new_col.set_index(\"new_name\")\n",
    "    df_new = new_col.join(df)\n",
    "    df_new = df_new.interpolate()\n",
    "    return df_new\n",
    "\n",
    "def get_gaze_from_csv(video_id, start_frame, window_size):\n",
    "    \"\"\"\n",
    "    :param video_id: the id of video you want to get openpose\n",
    "    :param start_frame: the frame when the bite action starts\n",
    "    :param window_size: the window size of each sample in seconds\n",
    "    \"\"\"\n",
    "    if os.path.exists(f\"/home/aa2375/social-dining/data/rt-gene/{video_id}_interp.csv\"):\n",
    "        gaze_df = pd.read_csv(f\"/home/aa2375/social-dining/data/rt-gene/{video_id}_interp.csv\")\n",
    "    else:\n",
    "        gaze_df = pd.read_csv(f\"/home/aa2375/social-dining/data/rt-gene/{video_id}.csv\")\n",
    "\n",
    "        gaze = np.array(gaze_df['gaze'])\n",
    "        head_pose = np.array(gaze_df['headpose'])\n",
    "        gazes = []\n",
    "        head_poses = []\n",
    "        for i in range(len(gaze)):\n",
    "            gazes.append(np.array(ast.literal_eval(gaze[i])))\n",
    "            head_poses.append(np.array(ast.literal_eval(head_pose[i])))\n",
    "        gazes = np.array(gazes)\n",
    "        head_poses = np.array(head_poses)\n",
    "\n",
    "        gaze_df['gaze_phi'] = gazes[:,0]\n",
    "        gaze_df['gaze_theta'] = gazes[:,1]\n",
    "        gaze_df['headpose_phi'] = head_poses[:,0]\n",
    "        gaze_df['headpose_theta'] = head_poses[:,1]\n",
    "        # delete the gaze and headpose ones\n",
    "        gaze_df = gaze_df.drop(columns=['gaze', 'headpose'])\n",
    "        gaze_df = interpolate_gaze_headpose(gaze_df)\n",
    "        gaze_df.to_csv(f\"/home/aa2375/social-dining/data/rt-gene/{video_id}_interp.csv\")\n",
    "\n",
    "    \n",
    "    start_id = start_frame\n",
    "    end_id = start_frame + 30 * window_size\n",
    "    \n",
    "    data = gaze_df.loc[start_id : end_id-1]\n",
    "    \n",
    "    gaze_phi = np.array(data['gaze_phi'])\n",
    "    gaze_theta = np.array(data['gaze_theta'])\n",
    "    headpose_phi = np.array(data['headpose_phi'])\n",
    "    headpose_theta = np.array(data['headpose_theta'])\n",
    "    gaze = np.stack([gaze_phi, gaze_theta],axis=1)\n",
    "    headpose = np.stack([headpose_phi, headpose_theta],axis=1)\n",
    "\n",
    "    return gaze, headpose\n",
    "\n",
    "gaze, headpose = get_gaze_from_csv('14_1', 64357, 6)\n",
    "print(gaze.shape)\n",
    "print(headpose.shape)\n",
    "\n",
    "# for i in range(1, 30+1):\n",
    "#     for j in range(1,3+1):\n",
    "#         name = str(i).zfill(2) + '_' +str(j)\n",
    "#         gaze, headpose = get_gaze_from_csv(name, 70, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1975dff5-910c-4023-bbaa-4b06b6c5d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH OH\n",
      "(0, 1) (0, 1) (0, 1)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Speaking features\n",
    "def get_speaking_from_csv(session_id, start_frame, window_size):\n",
    "    if session_id == '09':\n",
    "        return None, None, None\n",
    "    person_speaking = np.load(f\"/home/aa2375/social-dining/data/person-speaking/{session_id}.npy\")\n",
    "    one = np.array(person_speaking == 1)\n",
    "    two = np.array(person_speaking == 2)\n",
    "    three = np.array(person_speaking == 3)\n",
    "    \n",
    "    start_id = start_frame\n",
    "    end_id = start_frame + 30 * window_size\n",
    "    \n",
    "    one = one[start_id:end_id].reshape(-1,1)\n",
    "    two = two[start_id:end_id].reshape(-1,1)\n",
    "    three = three[start_id:end_id].reshape(-1,1)\n",
    "    if len(one) == 0 or len(two) == 0 or len(three) == 0:\n",
    "        print(\"UH OH\")\n",
    "    return one, two, three\n",
    "    \n",
    "\n",
    "one, two, three = get_speaking_from_csv('03', -5, 6)\n",
    "print(one.shape, two.shape, three.shape)\n",
    "print(len(one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd87330-1b6a-4be2-84e4-25dbd6585dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([1,0,1,1]) == 2).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5218e59-f000-45c1-8315-ddfcd249216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time features\n",
    "def get_time_from_csv(video_id, start_frame, window_size):\n",
    "    \"\"\"\n",
    "    :param video_id: the id of video you want to get openpose\n",
    "    :param start_frame: the frame when the bite action starts\n",
    "    :param window_size: the window size of each sample in seconds\n",
    "    \"\"\"\n",
    "    gaze_df = pd.read_csv(f\"/home/aa2375/social-dining/data/time-features/{video_id}.csv\")\n",
    "\n",
    "    \n",
    "    start_id = start_frame\n",
    "    end_id = start_frame + 30 * window_size\n",
    "    \n",
    "    data = gaze_df.loc[start_id : end_id-1]\n",
    "    \n",
    "    time_since_last_lifted = np.array(data['Time since last food_lifted  (s)'])\n",
    "    time_since_last_bite = np.array(data['Time since last food_to_mouth  (s)'])\n",
    "    time_since_last_bite_zeroed = np.array(data['Time since last food_to_mouth (zeroed within) (s)'])\n",
    "\n",
    "    return time_since_last_lifted, time_since_last_bite, time_since_last_bite_zeroed\n",
    "time_since_last_lifted, time_since_last_bite, time_since_last_bite_zeroed = get_time_from_csv('01_2', 70, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a9b7ab-2d1a-460e-8119-534925c3e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count features\n",
    "def get_count_from_csv(video_id, start_frame, window_size):\n",
    "    \"\"\"\n",
    "    :param video_id: the id of video you want to get openpose\n",
    "    :param start_frame: the frame when the bite action starts\n",
    "    :param window_size: the window size of each sample in seconds\n",
    "    \"\"\"\n",
    "    gaze_df = pd.read_csv(f\"/home/aa2375/social-dining/data/count-features/{video_id}.csv\")\n",
    "\n",
    "    \n",
    "    start_id = start_frame\n",
    "    end_id = start_frame + 30 * window_size\n",
    "    \n",
    "    data = gaze_df.loc[start_id : end_id-1]\n",
    "    \n",
    "    num_lifted = np.array(data['Number of food_lifted annotations'])\n",
    "    num_bites = np.array(data['Number of food_to_mouth annotations'])\n",
    "\n",
    "    return num_lifted, num_bites\n",
    "num_lifted, num_bites = get_count_from_csv('01_2', 70, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d950d2-6dcf-4315-a488-a9962dfd964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e94bbd-8e03-4eb1-80de-bac1252eb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load positive csv\n",
    "\n",
    "# add openposes to it\n",
    "# mapping_op_sample_to_elan_label('data/positive_labels.csv', 'positive_samples')\n",
    "\n",
    "# add gazes to it\n",
    "\n",
    "# add speaking to it\n",
    "\n",
    "# add count to it\n",
    "\n",
    "\n",
    "# load negative csv\n",
    "\n",
    "# add openposes to it\n",
    "# mapping_op_sample_to_elan_label('data/negative_labels.csv', 'negative_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7574c-0fc4-4c5c-8783-262b002af521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
